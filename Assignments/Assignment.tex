\documentclass[a4paper,11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{setspace}
\usepackage{hyperref}
\usepackage{listings}
    \lstset{basicstyle=\singlespacing\footnotesize\ttfamily}
\usepackage{comment}
    \excludecomment{solution}
    %\includecomment{solution}
\newcommand{\matr}[1]{\mathbf{#1}}

\title{Regression Computation Assignment}
\author{}
\date{}

\begin{document}

\maketitle

{\onehalfspacing

\section*{Purpose}

The purpose of this assignment is for you to demonstrate your understanding of the mathematics and computational procedures involved in the estimation of basic regression models. Below are several regression models. The first two (\texttt{m1} and \texttt{m2}) are models explaining life expectancy as a function of several covariates. The reported coefficient estimates were obtained using ordinary least squares (OLS) regression analysis using R's \texttt{lm} function. The third (\texttt{m3}) is a model explaining democracy as a function of several covariates, estimated using logistic regression via R's \texttt{glm} function. All data are drawn from the Quality of Government Basic Dataset. The data and codebook are available here: \url{http://qog.pol.gu.se/data/datadownloads/qogbasicdata}.

\section*{Overview}

Your task is to reproduce parts of these analyses using R (or Stata), without the use of the \texttt{lm}, \texttt{glm}, \texttt{lm.fit}, or similar ready-made functions (or, for Stata, without the use of \texttt{reg}, \texttt{logit}, \texttt{glm}, or similar commands). The instructions below outline the code you should produce. You may use R (based on techniques from lecture) or Stata (for relevant technical details on Stata, see Ch. 11 and Appendix A from Cameron and Trivedi 2010).
}

\section*{Submitting Your Assignment}

\noindent Submit your assignment via email to Thomas (\url{mailto:tleeper@ps.au.dk}) in the form of a single, complete R syntax file (.R) or Stata (.do) file in your submitted assignment. Each step of the code should be numbered (in the form of a comment) according to the numbering of the tasks. You do not need to explain, describe, or present the output of any analyses.

\vspace{1em}
\noindent The assignment is due on Thursday June 4 at 11:59.


\begin{enumerate}

\clearpage
\section{Ordinary Least Squares Regression}

\begin{lstlisting}
d <- rio::import("http://www.qogdata.pol.gu.se/data/qog_bas_cs_jan15.csv")
f1 <- une_leb ~ I(gle_cgdpc/1e4) + chga_demo + I(ross_oil_netexpc/1e3)
m1 <- lm(f1, data = d)

f2 <- une_leb ~ I(gle_cgdpc/1e4)*factor(chga_demo) + 
               I(ross_oil_netexpc/1e3) + factor(ht_colonial)
m2 <- lm(f2, data = d)
\end{lstlisting}

\item Extract the $\matr{R}$ matrix from the $\matr{Q}\matr{R}$ decomposition of model \texttt{m1}. Manually invert this matrix using the formula given in class for a 3-by-3 matrix.

\item Construct an appropriate design matrix for \texttt{m2} (as represented by formula \texttt{f2}) from the original variables in the dataset, including the specified categorical (factor) variables and interaction term. (See Fox p.153--154.)

\begin{solution}
\begin{lstlisting}

library('rio')

#model.matrix(~ 0 + factor(mtcars$cyl))

\end{lstlisting}
\end{solution}


\item Estimate the Ordinary Least Squares (OLS) coefficients in \texttt{m2} using matrix notation (see Fox p.155). You may use a QR decomposition in lieu of \texttt{solve} for matrix inversion if you so choose.

\begin{solution}
\begin{lstlisting}
# using standard matrix notation (X'X)^{-1}X'y
solve(t(X) %*% X) %*% t(X) %*% y

# using a QR decomposition
decomp <- qr(X)
solve(qr.R(decomp)) %*% t(qr.Q(decomp)) %*% y
\end{lstlisting}
\end{solution}

\item Obtain the fitted values, $\hat{y}$, from \texttt{m2} for a new observation that takes on the following values:

\begin{itemize}
\item \texttt{gle_cgdpc} = 10000
\item \texttt{chga_demo} = 1
\item \texttt{ross_oil_netexpc} = 1000
\item \texttt{ht_colonial} = 0
\end{itemize}

\begin{solution}
\begin{lstlisting}

\end{lstlisting}
\end{solution}

\item Estimate the variance-covariance matrix of the OLS estimates for \texttt{m2} and obtain estimated standard errors. The formula for OLS variance-covariance matrix is given on Fox p.158.

\begin{solution}
\begin{lstlisting}
# Var(beta) = S^2 (X'X)^{-1}
# S^2 = (e'e) / (n - k - 1)

fitted <- X %*% b
e <- fitted - y
df <- nrow(X) - length(b)
s2 <- (t(e) %*% e) / df

# variance-covariance matrix
V <- as.numeric(s2) * solve(t(X) %*% X)

# standard errors
sqrt(diag(V))
\end{lstlisting}
\end{solution}


\clearpage
\section{Logistic Regression}

\begin{lstlisting}
d <- rio::import("http://www.qogdata.pol.gu.se/data/qog_bas_cs_jan15.csv")
f3 <- chga_demo ~ al_ethnic + I(log(wdi_landarea)) + 
                 I(gle_cgdpc/1e4) + I(ross_oil_netexpc/1e3)
m3 <- glm(f3, data = d, family = binomial(link='logit'))
\end{lstlisting}


\item Represent the likelihood function for logistic regression as an R function (or Stata function). The formula for the likelihood function is:

$\sum_{i=1}^{n} y_i X\beta - X\beta - log(1 + e^{-X \beta})$

\begin{solution}
\begin{lstlisting}
logitll <- function(beta, outcome, covariates){
  xb <- covariates %*% beta
  sum(outcome * xb - xb - log(1 + exp(-xb)))
}
\end{lstlisting}
\end{solution}


\item Estimate coefficients for \texttt{m3} using two methods of maximum likelihood estimation: (a) a grid search and (b) an optimization algorithm (in R: \texttt{optim}).

\begin{solution}
\begin{lstlisting}
# grid search



# optimization algorithm
opt <- optim(par = rep(0, ncol(votes[,2:4]) + 1),
             fn = logit.ll,
             covariates = votes[,2:4],
             outcome = votes$incwin,
             control = list(fnscale = -1),
             hessian = T,
             method = "BFGS")
\end{lstlisting}
\end{solution}

\item Using \texttt{optim}, obtain the standard errors as the diagonal of the matrix inverse of the negative hessian matrix.

\begin{solution}
\begin{lstlisting}

\end{lstlisting}
\end{solution}


\item Re-estimate the standard errors for \texttt{m3} using bootstrapping and jackknife estimation.

\begin{solution}
\begin{lstlisting}
# function to estimate model one time
once <- function(X, y) {
    b <- solve(t(X) %*% X) %*% t(X) %*% y
    fitted <- X %*% b
    e <- fitted - y
    df <- nrow(X) - length(b)
    s2 <- (t(e) %*% e) / df
    V <- as.numeric(s2) * solve(t(X) %*% X)
    sqrt(diag(V))
}

# bootstrap
n <- 5000
bootmat <- matrix(numeric(), ncol = 3, nrow = n)
for(i in 1:n) {
    s <- sample(1:nrow(X), nrow(X), TRUE)
    Xtmp <- X[s,]
    ytmp <- y[s]
    tried <- try(once(Xtmp, ytmp))
    if(!inherits(tried, 'try-error'))
        bootmat[i,] <- tried
}
colMeans(bootmat, na.rm = TRUE)

# jackknife
jackmat <- matrix(numeric(), ncol = 3, nrow = nrow(X))
for(i in 1:nrow(X)) {
    Xtmp <- X[-i,]
    ytmp <- y[-i]
    tried <- try(once(Xtmp, ytmp))
    if(!inherits(tried, 'try-error'))
        jackmat[i,] <- tried
}
colMeans(jackmat, na.rm = TRUE)
\end{lstlisting}
\end{solution}


\item Compute the average predicted probability of observing the outcome  (i.e., the mean of all the predicted probability for each case in the original data) from your estimated coefficients for \texttt{m3}. (Do not use the \texttt{predict()} function, but you may wish to use it to check your answer.)

\begin{solution}
\begin{lstlisting}
odds <- as.numeric(exp(X %*% b))
prprob <- odds/(1+odds)
mean(prprob)
mean(predict(m3))
\end{lstlisting}
\end{solution}


\end{enumerate}


\end{document}
